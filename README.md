# TEAM-B--WASTE-GENERATION
"""WASTE_generation_onecodeforall.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v2tOVomQS6Jvwz6Jir0Kv1bXJHIhIbBy

Primary waste-df1

Secondary waste-df2

total waste-df3

soil contamination-df4

water pollution-df5

deforestation-df6

food waste-df7

householdwaste- df8

plasticwaste-df9

chemical waste- df10

economic cost- df11

environmental degradation- df12

public health risk- df13

landfill-df14

waste incineration-df15

production and disposal of plastic- df16

# GENERAL ANALYSIS

This code is created by Dharsheeni Kanthiah

Contents:

Primary waste- Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Secondary waste - Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Total waste- Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Organisation and integration all above waste categories sheet into one
"""

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import statistics

matplotlib.rcParams['figure.figsize'] = (12,8)

"""## Primary waste



"""

from google.colab import files
uploaded = files.upload()

### Data Exploration for Primary Waste

df1 = pd.read_csv('Primary waste.csv')
df1

df1.rename(columns={'Countries': 'Countries'}, inplace=True)
df1 = df1.set_index('Countries')

df1.describe()

#Displaying the shape of data frame and information summary
print('Primary Waste: ', df1.shape)
print(df1.info())

# Data types
print("Data Types")
print(df1.dtypes)

# Duplicate data
duplicates = df1[df1.duplicated()]
print("Duplicate Data:")
print(duplicates)

# Missing data per year
missing_data = df1.isnull().sum()
print("Missing Data:")
print(missing_data)

print()

# Histogram showing missing values for each YEAR
plt.figure(figsize=(5, 5))
missing_data.iloc[1:].plot(kind='bar', color='skyblue')
plt.title('Number of Missing Values per Year')
plt.xlabel('Years')
plt.ylabel('Number of Missing Values')
plt.show()
print()

# List of missing data count per country
missing_data_per_country = df1.isnull().sum(axis=1)
missing_data_per_country.index = df1['Countries']
print("Missing Data Count Per Country:")
print(missing_data_per_country)

print ()

# Heatmap for missing values
plt.figure(figsize=(6,8))
sns.heatmap(df1.iloc[:, 1:].isnull(), cmap='viridis', cbar=False, yticklabels=df1['Countries'])
plt.title('Heatmap of Missing Values')
plt.show()

# Selecting numeric columns for outliers detection
numeric_columns = df1.select_dtypes(include=['int', 'float']).columns

# Creating the box plot
plt.figure(figsize=(10, 6))
sns.boxplot(data=df1[numeric_columns], palette='viridis')
plt.title('Outliers detection for years')
plt.show()

###  Data Cleaning for Primary Waste

#Handling Missing Values

# Remove rows with the specified country names (case-insensitive)
countries_to_remove = ['Albania', 'Türkiye']
df1 = df1[~df1['Countries'].str.lower().isin([country.lower() for country in countries_to_remove])]



# Replacing other missing values with mean of the year values
df1.fillna(df1.mean(), inplace=True)

print()

#Displaying the dataset without missing values per year
missing_data = df1.isnull().sum()
print("AFTER Missing Data Replaced by mean of the year values:")
print(missing_data)

print()

# List of missing data count per country
missing_data_per_country = df1.isnull().sum(axis=1)
missing_data_per_country.index = df1['Countries']
print("Missing Data Count Per Country:")
print(missing_data_per_country)

# After handling missing values
print("After handling missing values:")
print(df1)

# Function to replace outliers in each row with the median of the row values
def replace_row_outliers_with_median(data, numeric_cols):
    for i, row in data.iterrows():
        q1 = row[numeric_cols].quantile(0.25)
        q3 = row[numeric_cols].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        data.loc[i, numeric_cols] = np.where((row[numeric_cols] < lower_bound) | (row[numeric_cols] > upper_bound),
                                             row[numeric_cols].median(), row[numeric_cols])
    return data

# Define numeric column names
numeric_cols = df1.columns[1:]


# Replace outliers at the row level with the median of each country's values
df1_cleaned = replace_row_outliers_with_median(df1.copy(), numeric_cols)

df1

###   Dataframe Organisation for Primary Waste

# Melt the DataFrame to convert from wide to long format
df1_new = pd.melt(df1_cleaned, id_vars=['Countries'], var_name='Year', value_name='Value')

# Display the long format DataFrame
print("Long Format DataFrame:")
df1_new

df1_new.to_csv('clean_primary_waste.csv')
files.download('clean_primary_waste.csv')

# Descriptive statistics
print("Descriptive Statistics:")
print(df1_new.describe())

# Correlation analysis
correlation_matrix = df1_cleaned.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()


# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': df1_cleaned.mean(),
    'Median': df1_cleaned.median(),
    'Mode': df1_cleaned.mode().iloc[0],
})
print("\nSummary Statistics:")
print(summary_stats)

# Bar plot showing waste values for specific countries
plt.figure(figsize=(12, 8))
sns.barplot(x='Value', y='Countries', data=df1_cleaned.melt(id_vars='Countries', var_name='Year', value_name='Value'),
            estimator='mean', ci=None, hue='Countries', palette='viridis')
plt.title('Country Comparison: Average Primary Waste Values')
plt.xlabel('Average Primary waste value in tonnes')
plt.ylabel('Countries')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# Omit the first two rows
df1_subset = df1_cleaned.iloc[2:]

# Bar plot showing waste values for specific countries
plt.figure(figsize=(12, 8))
sns.barplot(x='Value', y='Countries', data=df1_subset.melt(id_vars='Countries', var_name='Year', value_name='Value'),
            estimator='mean', ci=None, hue='Countries', palette='viridis')
plt.title('Country Comparison: Average Primary Waste Values')
plt.xlabel('Average Primary waste value in tonnes')
plt.ylabel('Countries')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""## Secondary waste"""

from google.colab import files
uploaded = files.upload()

###   Data Exploration for Secondary Waste

df2 = pd.read_csv('Secondary waste.csv')
df2

#Displaying the shape of data frame and information summary
print('Secondary Waste: ', df2.shape)
print(df2.info())

# Data types
print("Data Types")
print(df2.dtypes)

# Duplicate data
duplicates = df2[df2.duplicated()]
print("Duplicate Data:")
print(duplicates)

# Missing data per year
missing_data = df2.isnull().sum()
print("Missing Data:")
print(missing_data)

print()

# Histogram showing missing values for each YEAR
plt.figure(figsize=(5, 5))
missing_data.iloc[1:].plot(kind='bar', color='skyblue')
plt.title('Number of Missing Values per Year')
plt.xlabel('Years')
plt.ylabel('Number of Missing Values')
plt.show()
print()

# List of missing data count per country
missing_data_per_country = df2.isnull().sum(axis=1)
missing_data_per_country.index = df2['Countries']
print("Missing Data Count Per Country:")
print(missing_data_per_country)

print ()

# Heatmap for missing values
plt.figure(figsize=(8,8))
sns.heatmap(df2.iloc[:, 1:].isnull(), cmap='viridis', cbar=False, yticklabels=df2['Countries'])
plt.title('Heatmap of Missing Values')
plt.show()

#DETECTING OUTLIERS FOR EVERY YEAR

num_cols = df2.columns[1:]

# Setting up the subplots
plt.figure(figsize=(15, 8))
for i, col in enumerate(num_cols, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(x=col, data=df2)
    plt.title(f'Boxplot for {col}')

plt.tight_layout()
plt.show()

# Selecting numeric columns for outliers detection
numeric2_columns = df2.select_dtypes(include=['int', 'float']).columns

# Creating the box plot
plt.figure(figsize=(10, 6))
sns.boxplot(data=df2[numeric2_columns], palette='viridis')
plt.title('Outliers detection for years')
plt.show()

###   Data Cleaning for Secondary Waste

# Handling Missing Values

# Remove rows with the specified country names (case-insensitive)
countries_to_remove = ['Albania', 'Türkiye']
df1 = df1[~df1['Countries'].str.lower().isin([country.lower() for country in countries_to_remove])]


# Replacing other missing values with mean of the year values
df2.fillna(df2.mean(), inplace=True)

print()

#Displaying the dataset without missing values per year
missing_data = df2.isnull().sum()
print("AFTER Missing Data Replaced by mean of the year values:")
print(missing_data)

print()

# List of missing data count per country
missing_data_per_country = df2.isnull().sum(axis=1)
missing_data_per_country.index = df2['Countries']
print("Missing Data Count Per Country:")
print(missing_data_per_country)

print()

# After handling missing values
print("After handling missing values:")
print(df2)

df2

# Function to replace outliers in each row with the median of the row values
def replace_row_outliers_with_median(data):
    for i, row in data.iterrows():
        q1 = row[numeric_cols].quantile(0.25)
        q3 = row[numeric_cols].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        data.loc[i, numeric_cols] = np.where((row[numeric_cols] < lower_bound) | (row[numeric_cols] > upper_bound),
                                             row[numeric_cols].median(), row[numeric_cols])
    return data

# Replace outliers at the row level with the median of each country's values
df2_cleaned = replace_row_outliers_with_median(df2.copy())

# Displaying cleaned data with scatter plot
plt.figure(figsize=(12, 8))
for i, (index, row) in enumerate(df2_cleaned.iterrows()):
    plt.scatter([row['Countries']] * len(numeric_cols), row[numeric_cols], c='blue', marker='o')

plt.title('Outliers Replaced with Median (for Each Country)')
plt.xticks(ticks=np.arange(len(df2_cleaned)), labels=df2_cleaned['Countries'], rotation=45, ha="right")
plt.show()

###  Dataframe Organisation for Secondary Waste

# Melt the DataFrame to convert from wide to long format
df2_new = pd.melt(df2_cleaned, id_vars=['Countries'], var_name='Year', value_name='Value')

# Display the long format DataFrame
print("Long Format DataFrame:")
print(df2_new)

df2_new.to_csv('clean_secondary_waste.csv')
files.download('clean_secondary_waste.csv')

# Descriptive statistics
print("Descriptive Statistics:")
print(df2_cleaned.describe())

# Correlation analysis
correlation_matrix = df2_cleaned.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': df2_cleaned.mean(),
    'Median': df2_cleaned.median(),
    'Mode': df2_cleaned.mode().iloc[0],
})
print("\nSummary Statistics:")
print(summary_stats)

# Omit the first two rows
df2_subset = df2.iloc[2:]

# Bar plot showing waste values for specific countries
plt.figure(figsize=(12, 8))
sns.barplot(x='Value', y='Countries', data=df2_subset.melt(id_vars='Countries', var_name='Year', value_name='Value'),
            estimator='mean', ci=None, hue='Countries', palette='viridis')
plt.title('Country Comparison: Average Secondary Waste Values')
plt.xlabel('Average Secondary waste value in tonnes')
plt.ylabel('Countries')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""## Total waste



"""

from google.colab import files
uploaded = files.upload()

###   Data Exploration for Total Waste

df3 = pd.read_csv('Total waste.csv')
df3

#Removing the columns as we are analysing it from 2010 to 2020

df1.drop(df1.columns[[1, 2, 3]], axis=1, inplace =True)
df1

# Find the index location of columns to remove
columns_to_remove = ['2004', '2006', '2008']
indices_to_remove = [df3.columns.get_loc(col) for col in columns_to_remove]

# Drop the specified columns
df3.drop(df3.columns[indices_to_remove], axis=1, inplace=True)

# Display the DataFrame after removing columns
df3

#Displaying the shape of data frame and information summary
print('Total Waste: ', df3.shape)
print(df3.info())

# Data types
print("Data Types")
print(df3.dtypes)

# Duplicate data
duplicates = df3[df3.duplicated()]
print("Duplicate Data:")
print(duplicates)

# Missing data per year
missing_data = df3.isnull().sum()
print("Missing Data:")
print(missing_data)

print()

# Histogram showing missing values for each YEAR
plt.figure(figsize=(5, 5))
missing_data.iloc[1:].plot(kind='bar', color='skyblue')
plt.title('Number of Missing Values per Year')
plt.xlabel('Years')
plt.ylabel('Number of Missing Values')
plt.show()

# List of missing data count per country
missing_data_per_country = df3.isnull().sum(axis=1)
missing_data_per_country.index = df3['Countries']
print("Missing Data Count Per Country:")
print(missing_data_per_country)

print ()

# Heatmap for missing values
plt.figure(figsize=(8,8))
sns.heatmap(df3.iloc[:, 1:].isnull(), cmap='viridis', cbar=False, yticklabels=df3['Countries'])
plt.title('Heatmap of Missing Values')
plt.show()

#DETECTING OUTLIERS FOR EVERY YEAR

num_cols = df3.columns[1:]

# Setting up the subplots
plt.figure(figsize=(15, 8))
for i, col in enumerate(num_cols, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=col, data=df3)
    plt.title(f'Boxplot for {col}')

plt.tight_layout()
plt.show()

# Selecting numeric columns for outliers detection
numeric_cols = df3.columns[1:]

# Function to detect outliers for each row (country)
def detect_row_outliers(data):
    outliers = []
    for i, row in data.iterrows():
        q1 = row[numeric_cols].quantile(0.25)
        q3 = row[numeric_cols].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        row_outliers = row[numeric_cols][(row[numeric_cols] < lower_bound) | (row[numeric_cols] > upper_bound)].index.tolist()
        outliers.extend(row_outliers)
    return outliers

# Detect outliers at row level
row_outliers = detect_row_outliers(df3)

# Visualize outliers at row level with scatter plot
plt.figure(figsize=(12, 8))
for i, country in enumerate(df3['Countries']):
    plt.scatter([country] * len(row_outliers), df3.loc[i, row_outliers], c='red', marker='x')

plt.title('Outliers Detection (for Each Country)')
plt.xticks(ticks=np.arange(len(df3)), labels=df3['Countries'], rotation=45, ha="right")
plt.show()

###   Data Cleaning for Total Waste

# Handling Missing Values

# Remove rows with the country name specified (case-insensitive)
df3 =df3 = df3[~df3['Countries'].isin(['Albania', 'Turkiye'])]
print("After removing countries:")
print(df3)


# Replacing other missing values with mean of the year values
df3.fillna(df3.mean(), inplace=True)

print()

#Displaying the dataset without missing values per year
missing_data = df3.isnull().sum()
print("AFTER Missing Data Replaced by mean of the year values:")
print(missing_data)

print()

# List of missing data count per country
missing_data_per_country = df3.isnull().sum(axis=1)
missing_data_per_country.index = df3['Countries']
print("Missing Data Count Per Country:")
print(missing_data_per_country)

print()

# After handling missing values
print("After handling missing values:")
print(df3)

# Function to replace outliers in each row with the median of the row values
def replace_row_outliers_with_median(data):
    for i, row in data.iterrows():
        q1 = row[numeric_cols].quantile(0.25)
        q3 = row[numeric_cols].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        data.loc[i, numeric_cols] = np.where((row[numeric_cols] < lower_bound) | (row[numeric_cols] > upper_bound),
                                             row[numeric_cols].median(), row[numeric_cols])
    return data

# Replace outliers at the row level with the median of each country's values
df3_cleaned = replace_row_outliers_with_median(df3.copy())

# Displaying cleaned data with scatter plot
plt.figure(figsize=(12, 8))
for i, (index, row) in enumerate(df3_cleaned.iterrows()):
    plt.scatter([row['Countries']] * len(numeric_cols), row[numeric_cols], c='blue', marker='o')

plt.title('Outliers Replaced with Median (for Each Country)')
plt.xticks(ticks=np.arange(len(df3_cleaned)), labels=df3_cleaned['Countries'], rotation=45, ha="right")
plt.show()

###   Dataframe Organisation for Total Waste

# Melt the DataFrame to convert from wide to long format
df3_new = pd.melt(df3_cleaned, id_vars=['Countries'], var_name='Year', value_name='Value')

# Display the long format DataFrame
print("Long Format DataFrame:")
print(df3_new)


df3_new.to_csv('clean_total_waste.csv')
files.download('clean_total_waste.csv')

# Descriptive statistics
print("Descriptive Statistics:")
print(df3_cleaned.describe())

# Correlation analysis
correlation_matrix = df3_cleaned.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()


# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': df3_cleaned.mean(),
    'Median': df3_cleaned.median(),
    'Mode': df3_cleaned.mode().iloc[0],
})
print("\nSummary Statistics:")
print(summary_stats)

# Omit the first two rows
df3_subset = df3.iloc[2:]

# Bar plot showing waste values for specific countries
plt.figure(figsize=(12, 8))
sns.barplot(x='Value', y='Countries', data=df3_subset.melt(id_vars='Countries', var_name='Year', value_name='Value'),
            estimator='mean', ci=None, hue='Countries', palette='viridis')
plt.title('Country Comparison: Average Total Waste Values')
plt.xlabel('Average Total waste value in tonnes')
plt.ylabel('Countries')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""## Organisation and Integration for General analysis"""

uploaded = files.upload()

uploaded = files.upload()

uploaded = files.upload()

clean_df1 = pd.read_csv('clean_primary_waste (2).csv')
clean_df2 = pd.read_csv('clean_secondary_waste.csv')
clean_df3 = pd.read_csv('clean_total_waste (3).csv')

# Data integration
general_df = pd.merge(clean_df1, clean_df2, on=['Countries', 'Year'])

# Data integration
general_df = pd.merge(general_df, clean_df3, on=['Countries', 'Year'])

with pd.option_context('display.max_rows', None, 'display.max_columns', None):
  general_df
general_df

missing_values = general_df.isnull().sum()
missing_values

# Rename columns
general_df = general_df.rename(columns={'Value_x': 'Primary Waste', 'Value_y': 'Secondary Waste', 'Value': 'Total Waste'})


# Get the column names that contain 'Unnamed'
unnamed_columns = [col for col in general_df.columns if 'Unnamed' in col]

# Drop the columns containing 'Unnamed'
general_df= general_df.drop(columns=unnamed_columns)

# Display the DataFrame after removing the columns and renaming
print(general_df)

general_df

general_df.to_csv('general_analysis.csv')
files.download('general_analysis.csv')

general_df.describe()

# Descriptive statistics
summary_stats = pd.DataFrame({
    'Mean': general_df.mean(),
    'Median': general_df.median(),
    'Mode': general_df.mode().iloc[0],
})
print("\Summary Statistics:")
print(summary_stats)

# Drop the 'Year' column from the DataFrame
general_df_without_year = general_df.drop(columns=['Year'])

# Compute the correlation matrix
correlation_matrix = general_df_without_year.corr()

# Plot the correlation matrix
plt.figure(figsize=(8, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of all general waste categories')
plt.show()



"""# ON ENVIRONMENT ANALYSIS

This code is created by Syed Haris

Contents:

Soil Contamination - Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Water Pollution - Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Deforestation- Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Organisation and integration all above waste categories sheet into one


"""

from google.colab import files
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import statistics



"""## **Soil Contamination**"""

uploaded = files.upload()

df4 = pd.read_csv('Soil contamination.csv')
df4.info()

df4.rename(columns={'TIME': 'COUNTRIES'}, inplace=True)

df4 = df4.set_index('COUNTRIES')

# Set style
sns.set(style="whitegrid")

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df4.isnull(), cbar=False, cmap='viridis')
plt.title('Missing soil Values Heatmap')
plt.show()

df4.at['European Union - 28 countries (2013-2020)', '2020'] = 530610000.0

average = (47 + 65) / 2
average = round(average)
df4.at['Malta', '2012'] = average
df4.at['Malta', '2014'] = average
df4.at['Malta', '2016'] = average
df4.at['Malta', '2020'] = average
df4

df4.at['Norway', '2010'] = 3920.0

df4.at['United Kingdom', '2020'] = 62009410.0

average_her = (df4.at['Bosnia and Herzegovina', '2012'] + df4.at['Bosnia and Herzegovina', '2014'] + df4.at['Bosnia and Herzegovina', '2016'] + df4.at['Bosnia and Herzegovina', '2018'] + df4.at['Bosnia and Herzegovina', '2020']) / 5
average_her = round(average_her)
df4.at['Bosnia and Herzegovina', '2010'] = average_her

df4.at['Montenegro', '2010'] = 101918.0

df4 = df4.drop('Albania')

df4.at['Kosovo*', '2010'] = 1045.0
df4.at['Kosovo*', '2012'] = 1045.0
df4.info()
df4

df4.info()

df4.to_csv('clean_soil_contamination.csv')
files.download('clean_soil_contamination.csv')

# Reset the index of df4_subset
df4_subset = df4.iloc[2:]
df4_subset_reset = df4_subset.reset_index()

# Bar plot showing waste values for specific countries
plt.figure(figsize=(12, 8))
sns.barplot(x='Value', y='COUNTRIES', data=df4_subset_reset.melt(id_vars='COUNTRIES', var_name='Year', value_name='Value'),
            estimator='mean', ci=None, hue='COUNTRIES', palette='viridis')
plt.title('Soil contamination values per country')
plt.xlabel('Average soil contamination value in tonnes')
plt.ylabel('Countries')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""## Water pollution"""

uploaded = files.upload()

df5 = pd.read_csv('Water pollution.csv')
df5.info()
df5

df5.rename(columns={'TIME': 'COUNTRIES'}, inplace=True)

df5 = df5.set_index('COUNTRIES')

# Set style
sns.set(style="whitegrid")

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df5.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

df5 = df5.drop('Albania')

df5.at['European Union - 28 countries (2013-2020)', '2020'] = 10200000.0

values = (
    df5.at['Latvia', '2012'],
    df5.at['Latvia', '2014'],
    df5.at['Latvia', '2016'],
    df5.at['Latvia', '2018'],
    df5.at['Latvia', '2020']
)
median = statistics.median(values)
median = round(median)
df5.at['Latvia', '2010'] = median

average = (df5.at['Iceland', '2010'] + df5.at['Iceland', '2018']) / 2
average = round(average)
df5.at['Iceland', '2012'] = average
df5.at['Iceland', '2014'] = average
df5.at['Iceland', '2016'] = average
df5.at['Iceland', '2020'] = average

values = (
    df5.at['Liechtenstein', '2014'],
    df5.at['Liechtenstein', '2016'],
    df5.at['Liechtenstein', '2018'],
    df5.at['Liechtenstein', '2020']
)
median = statistics.median(values)
median = round(median)
df5.at['Liechtenstein', '2010'] = median
df5.at['Liechtenstein', '2012'] = median
df5

average = (df5.at['Norway', '2018'] + df5.at['Norway', '2020']) / 2
average = round(average)
df5.at['Norway', '2010'] = average
df5.at['Norway', '2012'] = average
df5.at['Norway', '2014'] = average
df5.at['Norway', '2016'] = average

average = (df5.at['United Kingdom', '2010'] + df5.at['United Kingdom', '2012'] + df5.at['United Kingdom', '2014'] + df5.at['United Kingdom', '2016'] + df5.at['United Kingdom', '2018']) / 5
average = round(average)
df5.at['United Kingdom', '2020'] = average

df5.at['Bosnia and Herzegovina', '2010'] = 10.0
df5.at['Bosnia and Herzegovina', '2012'] = 10.0
df5.at['Bosnia and Herzegovina', '2014'] = 10.0
df5.at['Bosnia and Herzegovina', '2018'] = 10.0
df5.at['Bosnia and Herzegovina', '2020'] = 10.0

average = (df5.at['Montenegro', '2018'] + df5.at['Montenegro', '2020']) / 2
average = round(average)
df5.at['Montenegro', '2010'] = average
df5.at['Montenegro', '2012'] = average
df5.at['Montenegro', '2014'] = average
df5.at['Montenegro', '2016'] = average
df5

values = (
    df5.at['North Macedonia', '2014'],
    df5.at['North Macedonia', '2016'],
    df5.at['North Macedonia', '2018']
)
median = statistics.median(values)
median = round(median)
df5.at['North Macedonia', '2010'] = median
df5.at['North Macedonia', '2012'] = median
df5.at['North Macedonia', '2020'] = median

df5.at['Kosovo*', '2010'] = 1.0
df5.at['Kosovo*', '2014'] = 1.0
df5.at['Kosovo*', '2016'] = 1.0
df5.at['Kosovo*', '2020'] = 1.0
df5

df5.info()

df5.to_csv('clean_water_pollution.csv')
files.download('clean_water_pollution.csv')

# Reset the index of df5_subset
df5_subset = df5.iloc[2:]
df5_subset_reset = df5_subset.reset_index()

# Bar plot showing waste values for specific countries
plt.figure(figsize=(12, 8))
sns.barplot(x='Value', y='COUNTRIES', data=df4_subset_reset.melt(id_vars='COUNTRIES', var_name='Year', value_name='Value'),
            estimator='mean', ci=None, hue='COUNTRIES', palette='viridis')
plt.title('water pollution values per country')
plt.xlabel('Average water pollution value in tonnes')
plt.ylabel('Countries')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""## Deforestation"""

uploaded = files.upload()

df6 = pd.read_csv('Deforestation.csv')
df6

df6.rename(columns={'TIME': 'COUNTRIES'}, inplace=True)

df6 = df6.set_index('COUNTRIES')

columns_to_remove = ['2004', '2006', '2008']
df6 = df6.drop(columns=columns_to_remove)
df6

# Set style
sns.set(style="whitegrid")

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df6.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

df6.info()

df6 = df6.drop('Albania')

average = (df6.at['European Union - 28 countries (2013-2020)', '2010'] +
           df6.at['European Union - 28 countries (2013-2020)', '2012'] +
           df6.at['European Union - 28 countries (2013-2020)', '2014'] +
           df6.at['European Union - 28 countries (2013-2020)', '2016'] +
           df6.at['European Union - 28 countries (2013-2020)', '2018']) / 5
average = round(average)
df6.at['European Union - 28 countries (2013-2020)', '2020'] = average

df6.at['United Kingdom', '2020'] = 7467562.0

average = (df6.at['Bosnia and Herzegovina', '2012'] +
           df6.at['Bosnia and Herzegovina', '2014'] +
           df6.at['Bosnia and Herzegovina', '2016'] +
           df6.at['Bosnia and Herzegovina', '2018'] +
           df6.at['Bosnia and Herzegovina', '2020']) / 5
average = round(average)
df6.at['Bosnia and Herzegovina', '2010'] = average
df6

values = (
    df6.at['Montenegro', '2012'],
    df6.at['Montenegro', '2014'],
    df6.at['Montenegro', '2016'],
    df6.at['Montenegro', '2018'],
    df6.at['Montenegro', '2020'],
)
median = statistics.median(values)
median = round(median)
df6.at['Montenegro', '2010'] = median

values = (
    df6.at['Kosovo*', '2012'],
    df6.at['Kosovo*', '2014'],
    df6.at['Kosovo*', '2016'],
    df6.at['Kosovo*', '2018'],
    df6.at['Kosovo*', '2020'],
)
median = statistics.median(values)
median = round(median)
df6.at['Kosovo*', '2010'] = median
df6

df6.info()

df6.to_csv('clean_Deforestation.csv')
files.download('clean_Deforestation.csv')

# Reset the index of df6_subset
df6_subset = df6.iloc[2:]
df6_subset_reset = df6_subset.reset_index()

# Bar plot showing waste values for specific countries
plt.figure(figsize=(12, 8))
sns.barplot(x='Value', y='COUNTRIES', data=df6_subset_reset.melt(id_vars='COUNTRIES', var_name='Year', value_name='Value'),
            estimator='mean', ci=None, hue='COUNTRIES', palette='viridis')
plt.title('Deforestation values per country')
plt.xlabel('Average deforestation value in tonnes')
plt.ylabel('Countries')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""## Organisation and integration for enviroment"""

uploaded = files.upload()

uploaded = files.upload()

uploaded = files.upload()

clean_df4 = pd.read_csv('clean_Deforestation (1).csv')
clean_df5 = pd.read_csv('clean_water_pollution (2).csv')
clean_df6 = pd.read_csv('clean_soil_contamination (2).csv')

# Convert water pollution dataset to long format
water_pollution_long = df5.reset_index().melt(id_vars='COUNTRIES', var_name='year', value_name='water_pollution')

# Convert deforestation dataset to long format
deforestation_long = df4.reset_index().melt(id_vars='COUNTRIES', var_name='year', value_name='deforestation')

# Convert soil contamination dataset to long format
soil_contamination_long = df6.reset_index().melt(id_vars='COUNTRIES', var_name='year', value_name='soil_contamination')

# Merge water pollution and deforestation datasets
enviroment_df = pd.merge(water_pollution_long, deforestation_long, on=['COUNTRIES', 'year'])

# Merge with soil contamination dataset
enviroment_df = pd.merge(enviroment_df, soil_contamination_long, on=['COUNTRIES', 'year'])

enviroment_df

enviroment_df.head(50)

enviroment_df.to_csv('enviroment.csv')
files.download('enviroment.csv')

# Drop the 'Year' column from the DataFrame
general_df_without_year = general_df.drop(columns=['Year'])

# Compute the correlation matrix
correlation_matrix = general_df_without_year.corr()

# Plot the correlation matrix
plt.figure(figsize=(8, 5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of all general waste categories')
plt.show()



"""# ON HEALTH ANALYSIS

This code is created by Nency Kukadiya

Contents:

FOOD WASTE- Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

HOUSEHOLD WASTE - Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

PLASTIC WASTE- Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

CHEMICAL WASTE- Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Organisation and integration all above waste categories sheet into one
"""

from google.colab import files
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import statistics

"""## **Food Waste**"""

uploaded = files.upload()

df7 = pd.read_csv('Food waste.csv')
df7

#Changing 'TIME' column to 'COUNTRY'
df7.rename(columns={'TIME': 'COUNTRIES'}, inplace=True)
df7.info()

df7.describe()

duplicates = df7[df7.duplicated()]
print("Duplicate data:")
print(duplicates)

missing_values = df7.isnull().sum()
print("Missing Values:")
print(missing_values)

#Histogram to show missing values
plt.figure(figsize=(5,5))
missing_values.iloc[1:].plot(kind='bar')
plt.title('Count of Missing Values from 2010 to 2020')
plt.xlabel('Years')
plt.ylabel('Missing Values Count')
plt.show()
print()

#List of missing data per country
missing_per_country = df7.isnull().sum(axis=1)
missing_per_country.index = df7['COUNTRIES']
print("Missing values per country:")
print(missing_per_country)

# Heatmap for missing values per country
plt.figure(figsize=(10,8))
sns.heatmap(df7.iloc[:, 1:].isnull(),cmap='viridis', cbar=False, yticklabels=df7['COUNTRIES'])
plt.title('Heatmap of missing values')
plt.show()

# Outliers detection
numeric_columns = df7.select_dtypes(include=['int', 'float']).columns
plt.figure(figsize=(5,5))
sns.boxplot(data=df7.iloc[:,1:][numeric_columns], palette='viridis')
plt.title('Boxplot for outliers')
plt.show()

#Handling outliers
z_score_threshold = 3
z_scores = stats.zscore(df7[numeric_columns])
z_scores_df = pd.DataFrame(z_scores, columns=numeric_columns)
outliers_mask = (z_scores_df.abs() > z_score_threshold).any(axis=1)
dataframe_no_outliers = df7[~outliers_mask]
print(dataframe_no_outliers)

#Handling missing values
#Dropping the empty or non-useful rows
df7 = df7.drop(21)
df7 = df7.drop(36)

#setting the index as removed some rows
df7 = df7.reset_index(drop=True)

missing_values = df7.isnull().sum()
missing_values

# Set style
sns.set(style="whitegrid")

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df7.isnull(), cbar=False, cmap='viridis')
plt.title('Food waste Missing Values Heatmap')
plt.show()

years = ['2010', '2012', '2014', '2016', '2018', '2020']
data = df7[years]

# Create a box plot
data.boxplot(grid=False, vert=True, figsize=(10, 6))
plt.title('Box Plot of Food Waste across Years')
plt.ylabel('Food Waste')
plt.xlabel('Years')
plt.show()

missing_pct = round(df7.isnull().sum()/len(df7)*100,1)
print(missing_pct)

#since all the missing values are below 30%, we can fill them with mean, median.

#put the value of 2018 for 2020 for european union since the values are massive and rise gradually
df7.at[1, '2020'] = 16130000.0

#Fill average for united kingdom, bosnia, Montenegro, Kosovo,  Liechtenstien
average_UnitedKingdom = (df7.at[31, '2010'] + df7.at[31, '2012'] + df7.at[31, '2014'] + df7.at[31, '2016'] + df7.at[31, '2018']) / 5
df7.at[31, '2020'] = average_UnitedKingdom

average_Bosnia = (df7.at[32, '2012'] + df7.at[32, '2014'] + df7.at[32, '2016'] + df7.at[32, '2018'] + df7.at[32, '2020']) / 5
df7.at[32, '2010'] = average_Bosnia

average_Montenegro = (df7.at[33, '2012'] + df7.at[33, '2014'] + df7.at[33, '2016'] + df7.at[33, '2018'] + df7.at[33, '2020']) / 5
df7.at[33, '2010'] = average_Montenegro

average_Kosovo = (df7.at[37, '2012'] + df7.at[37, '2014'] + df7.at[37, '2016'] + df7.at[37, '2018'] + df7.at[37, '2020']) / 5
df7.at[37, '2010'] = average_Kosovo

average_Liechtenstein = (df7.at[29, '2012'] + df7.at[29, '2014'] + df7.at[29, '2016'] + df7.at[29, '2018'] + df7.at[29, '2020']) / 5
df7.at[29, '2010'] = average_Liechtenstein

# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': df7.mean(),
    'Median': df7.median(),
    'Mode': df7.mode().iloc[0],
})
print("\Summary Statistics:")
print(summary_stats)

#Correlation metrics after cleaning data

correlation_matrix = df7.corr()
plt.figure(figsize=(5,5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix After Cleaning Data')
plt.show()

# Food Wastage per country
df7['Total Wastage'] = df7.iloc[:, 1:].sum(axis=1)
plt.figure(figsize=(10,8))
sns.barplot(x='Total Wastage', y='COUNTRIES', data=df7, palette='viridis')
plt.title('Total Wastage per Country')
plt.xlabel('Total Wastage')
plt.ylabel('Country')
plt.show()

missing_values = df7.isnull().sum()
missing_values

df7.to_csv('Food waste.csv', index=False)
files.download('Food waste.csv')

df7

"""## **Household Waste**"""

uploaded = files.upload()

df8 = pd.read_csv('Household waste.csv')
df8

#Removing the columns as we are analysing it from 2010 to 2020

df8.drop(df8.columns[[1, 2, 3]], axis=1, inplace =True)

#Change the time column to countries

df8.rename(columns={'TIME': 'COUNTRIES'}, inplace=True)
df8.info()

duplicates = df8[df8.duplicated()]
print("Duplicate data:")
print(duplicates)

missing_values = df8.isnull().sum()
print("Missing Values:")
print(missing_values)

#Histogram to show missing values

plt.figure(figsize=(5,5))
missing_values.iloc[1:].plot(kind='bar')
plt.title('Count of Missing Values from 2010 to 2020')
plt.xlabel('Years')
plt.ylabel('Missing Values Count')
plt.show()
print()

#List of missing data per country
missing_per_country = df8.isnull().sum(axis=1)
missing_per_country.index = df8['COUNTRIES']
print("Missing values per country:")
print(missing_per_country)

# Heatmap for missing values per country
plt.figure(figsize=(10,8))
sns.heatmap(df8.iloc[:, 1:].isnull(),cmap='viridis', cbar=False, yticklabels=df8['COUNTRIES'])
plt.title('Heatmap of missing values')
plt.show()

# Outliers detection
numeric_columns = df8.select_dtypes(include=['int', 'float']).columns
plt.figure(figsize=(5,5))
sns.boxplot(data=df8.iloc[:,1:][numeric_columns], palette='viridis')
plt.title('Boxplot for outliers')
plt.show()

missing_values = df8.isnull().sum()
missing_values

missing_pct = round(df8.isnull().sum()/len(df8)*100,1)
print(missing_pct)

#Since all the missing values are below 30%, we can fill them with mean, median.

df8.describe()

#Dropping the row since many missing data

df8 = df8.drop(36)

#Resetting the index as dropped one row

df8 = df8.reset_index(drop=True)

#Ploting the histogram of missing values

# Set style
sns.set(style="whitegrid")

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df8.isnull(), cbar=False, cmap='viridis')
plt.title('Household waste Missing Values Heatmap')
plt.show()

years = ['2010', '2012', '2014', '2016', '2018', '2020']
data = df8[years]

data.boxplot(grid=False, vert=True, figsize=(10, 6))
plt.title('Box Plot of household Waste across Years')
plt.ylabel('Household Waste')
plt.xlabel('Years')
plt.show()

#put the value of 2018 for 2020 for european union since the values are massive and rise gradually

df8.at[1, '2020'] = 165750000.0

#Fill average for United Kingdom,montenegro

average_UnitedKingdom = (df8.at[32, '2010'] + df8.at[32, '2012'] + df8.at[32, '2014'] + df8.at[32, '2016'] + df8.at[32, '2018']) / 5
df8.at[32, '2020'] = average_UnitedKingdom


average_montenegro = (df8.at[34, '2012'] + df8.at[34, '2014'] + df8.at[34, '2016'] + df8.at[34, '2018'] + df8.at[34, '2020']) / 5
average_montenegro = round(average_montenegro)
df8.at[34, '2010'] = average_montenegro
average_mont = round(average_montenegro)

#Fill bosnia with median since it has outliers

values = [
    df8.at[33, '2012'],
    df8.at[33, '2014'],
    df8.at[33, '2016'],
    df8.at[33, '2018'],
    df8.at[33, '2020']
]
median_bosnia = statistics.median(values)

df8.at[33,'2010'] = median_bosnia


#Fill median for kosovo

values3 = [
    df8.at[38, '2012'],
    df8.at[38, '2014'],
    df8.at[38, '2016'],
    df8.at[38, '2018'],
    df8.at[38, '2020']
]
median_kosovo = statistics.median(values3)

df8.at[38, '2010'] = median_kosovo

# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': df8.mean(),
    'Median': df8.median(),
    'Mode': df8.mode().iloc[0],
})
print("\Summary Statistics:")
print(summary_stats)

#Correlation metrics after cleaning data

correlation_matrix = df8.corr()
plt.figure(figsize=(8,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix After Cleaning Data')
plt.show()

#Household Wastage per country

df8['Total Wastage'] = df8.iloc[:, 1:].sum(axis=1)

plt.figure(figsize=(10,8))
sns.barplot(x='Total Wastage', y='COUNTRIES', data=df8, palette='viridis')
plt.title('Total Wastage per Country')
plt.xlabel('Total Wastage')
plt.ylabel('Country')
plt.show()

#New metrics

missing_values = df8.isnull().sum()
missing_values

df8.to_csv('Household waste.csv', index=False)
files.download('Household waste.csv')

"""## **Plastic Waste**"""

uploaded = files.upload()

df9 = pd.read_csv('Plastic waste.csv')
df9

#removing columns as we are only analysing from 2010-2020

df9.drop(df9.columns[[1, 2, 3]], axis=1, inplace =True)

#change the time column to countries

df9.rename(columns={'TIME': 'COUNTRIES'}, inplace=True)
df9.info()

duplicates = df9[df9.duplicated()]
print("Duplicate data:")
print(duplicates)

missing_values = df9.isnull().sum()
print("Missing Values:")
print(missing_values)

#Histogram to show missing values

plt.figure(figsize=(5,5))
missing_values.iloc[1:].plot(kind='bar')
plt.title('Count of Missing Values from 2010 to 2020')
plt.xlabel('Years')
plt.ylabel('Missing Values Count')
plt.show()
print()

#List of missing data per country
missing_per_country = df9.isnull().sum(axis=1)
missing_per_country.index = df9['COUNTRIES']
print("Missing values per country:")
print(missing_per_country)

# Heatmap for missing values per country
plt.figure(figsize=(10,8))
sns.heatmap(df9.iloc[:, 1:].isnull(),cmap='viridis', cbar=False, yticklabels=df9['COUNTRIES'])
plt.title('Heatmap of missing values')
plt.show()

# Outliers detection
numeric_columns = df9.select_dtypes(include=['int', 'float']).columns
plt.figure(figsize=(5,5))
sns.boxplot(data=df9.iloc[:,1:][numeric_columns], palette='viridis')
plt.title('Boxplot for outliers')
plt.show()

missing_values = df9.isnull().sum()
missing_values

missing_pct = round(df9.isnull().sum()/len(df9)*100,1)
print(missing_pct)

#Since all the missing values are below 30%, we can fill them with mean, median

df9.describe()

#Remove Albania since no data

df9 = df9.drop(36)

#Reset index

df9 = df9.reset_index(drop=True)

#Put the value of 2018 for 2020 for european union since the values are massive and rise gradually

df9.at[1, '2020'] = 20160000.0

#Fill average for UK, montenegro

average_uk = (df9.at[32, '2010'] + df9.at[32, '2012'] + df9.at[32, '2014'] + df9.at[32, '2016'] + df9.at[32, '2018']) / 5
average_uk = round(average_uk)
print(average_uk)

df9.at[32, '2020'] = average_uk

average_mont = (df9.at[34, '2012'] + df9.at[34, '2014'] + df9.at[34, '2016'] + df9.at[34, '2018'] + df9.at[34, '2020']) / 5
average_mont = round(average_mont)
df9.at[34, '2010'] = average_mont
average_mont = round(average_mont)

#Fill median for bosnia, kosovo
values = [
    df9.at[33, '2012'],
    df9.at[33, '2014'],
    df9.at[33, '2016'],
    df9.at[33, '2018'],
    df9.at[33, '2020']
]
median_bosnia = statistics.median(values)

df9.at[33,'2010'] = median_bosnia

values3 = [
    df9.at[38, '2012'],
    df9.at[38, '2014'],
    df9.at[38, '2016'],
    df9.at[38, '2018'],
    df9.at[38, '2020']
]
median_kos = statistics.median(values3)

df9.at[38, '2010'] = median_kos

# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': df9.mean(),
    'Median': df9.median(),
    'Mode': df9.mode().iloc[0],
})
print("\Summary Statistics:")
print(summary_stats)

#Correlation metrics after cleaning data

correlation_matrix = df9.corr()
plt.figure(figsize=(8,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix After Cleaning Data')
plt.show()

#Plastic Wastage per country

df9['Total Wastage'] = df9.iloc[:, 1:].sum(axis=1)

plt.figure(figsize=(10,8))
sns.barplot(x='Total Wastage', y='COUNTRIES', data=df9, palette='viridis')
plt.title('Total Wastage per Country')
plt.xlabel('Total Wastage')
plt.ylabel('Country')
plt.show()

#New metrix

missing_values = df9.isnull().sum()
missing_values

df9.to_csv('plastic waste.csv', index=False)
files.download('plastic waste.csv')

df9

"""## **Chemical waste**"""

uploaded = files.upload()

df10 = pd.read_csv('Chemical waste.csv')
df10

#Change time to countries as a column name

df10.rename(columns={'TIME': 'COUNTRIES'}, inplace=True)
df10.info()

duplicates = df10[df10.duplicated()]
print("Duplicate data:")
print(duplicates)

missing_values = df10.isnull().sum()
print("Missing Values:")
print(missing_values)

#Histogram to show missing values

plt.figure(figsize=(5,5))
missing_values.iloc[1:].plot(kind='bar')
plt.title('Count of Missing Values from 2010 to 2020')
plt.xlabel('Years')
plt.ylabel('Missing Values Count')
plt.show()
print()

#List of missing data per country
missing_per_country = df10.isnull().sum(axis=1)
missing_per_country.index = df10['COUNTRIES']
print("Missing values per country:")
print(missing_per_country)

# Heatmap for missing values per country
plt.figure(figsize=(10,8))
sns.heatmap(df10.iloc[:, 1:].isnull(),cmap='viridis', cbar=False, yticklabels=df10['COUNTRIES'])
plt.title('Heatmap of missing values')
plt.show()

# Outliers detection
numeric_columns = df10.select_dtypes(include=['int', 'float']).columns
plt.figure(figsize=(5,5))
sns.boxplot(data=df10.iloc[:,1:][numeric_columns], palette='viridis')
plt.title('Boxplot for outliers')
plt.show()

missing_values = df10.isnull().sum()
missing_values

missing_pct = round(df10.isnull().sum()/len(df10)*100,1)
print(missing_pct)

#Since all the missing values are below 30%, we can fill them with mean, median.

df10.describe()

#Drop rows with no values

df10= df10.drop(21)
df10 = df10.drop(36)

#Reset index
df10 = df10.reset_index(drop=True)

#Fill with average UK,montenegro, european union, Liechtenstien

average_UK = (df10.at[31, '2012'] + df10.at[31, '2014'] + df10.at[31, '2016'] + df10.at[31, '2018'] + df10.at[31, '2010']) / 5
average_UK = round(average_UK)
df10.at[31, '2020'] = average_UK

average_mon = (df10.at[33, '2012'] + df10.at[33, '2014'] + df10.at[33, '2016'] + df10.at[33, '2018'] + df10.at[33, '2020']) / 5
average_mon = round(average_mon)
df10.at[33, '2010'] = average_mon

average_eur = (df10.at[1, '2012'] + df10.at[1, '2014'] + df10.at[1, '2016'] + df10.at[1, '2018'] + df10.at[1, '2010']) / 5
average_eur = round(average_eur)
df10.at[1, '2020'] = average_eur

average_Liechtenstein = (df10.at[29, '2012'] + df10.at[29, '2014'] + df10.at[29, '2016'] + df10.at[29, '2018'] + df10.at[29, '2020']) / 5
df10.at[29, '2010'] = average_Liechtenstein

#Fill with median bosnia, kosovo

values = [
    df10.at[32, '2012'],
    df10.at[32, '2014'],
    df10.at[32, '2016'],
    df10.at[32, '2018'],
    df10.at[32, '2020']
]
median = statistics.median(values)
median = round(median)
df10.at[32, '2010'] = median



values = [
    df10.at[37, '2012'],
    df10.at[37, '2014'],
    df10.at[37, '2016'],
    df10.at[37, '2018'],
    df10.at[37, '2020']
]
median = statistics.median(values)
median = round(median)
df10.at[37, '2010'] = median

# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': df10.mean(),
    'Median': df10.median(),
    'Mode': df10.mode().iloc[0],
})
print("\Summary Statistics:")
print(summary_stats)

#Correlation after cleaning data

correlation_matrix = df10.corr()
plt.figure(figsize=(8,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix After Cleaning Data')
plt.show()

#Chemical Wastage per country

df10['Total Wastage'] = df10.iloc[:, 1:].sum(axis=1)

plt.figure(figsize=(10,8))
sns.barplot(x='Total Wastage', y='COUNTRIES', data=df10, palette='viridis')
plt.title('Total Wastage per Country')
plt.xlabel('Total Wastage')
plt.ylabel('Country')
plt.show()

#New metrix

missing_values = df10.isnull().sum()
missing_values

df10.to_csv('chemical waste.csv', index=False)
files.download('chemical waste.csv')

"""## **Organisation and integration for health**"""

df7 = pd.read_csv("Food waste.csv")
df8 = pd.read_csv("Household waste.csv")
df9 = pd.read_csv("Plastic waste.csv")
df10 = pd.read_csv("Chemical waste.csv")

# Convert Food waste dataset to long format
Food_waste_long = df7.reset_index().melt(id_vars = "COUNTRIES", var_name = "year", value_name = "Food waste")

# Convert Household waste dataset to long format
Household_waste_long = df8.reset_index().melt(id_vars = "COUNTRIES", var_name = "year", value_name = "Household waste")

# Convert Plastic waste dataset to long format
Plastic_waste_long = df9.reset_index().melt(id_vars = "COUNTRIES", var_name = "year", value_name = "Plastic waste")

# Convert Chemical waste dataset to long format
Chemical_waste_long = df10.reset_index().melt(id_vars = "COUNTRIES", var_name = "year", value_name = "Chemical waste")

# Merge Food waste and household waste datasets
merged_df_Health = pd.merge(Food_waste_long,Household_waste_long,on=['COUNTRIES','year'])

# Merge with Plastic waste
merged_df_Health = pd.merge(merged_df_Health,Plastic_waste_long, on=['COUNTRIES','year'])

# Merge with Chemical waste
merged_df_Health = pd.merge(merged_df_Health,Chemical_waste_long,on=['COUNTRIES','year'])

merged_df_Health

with pd.option_context('display.max_rows', None, 'display.max_columns', None):
  merged_df_Health
merged_df_Health

missing_values = merged_df_Health.isnull().sum()
missing_values

merged_df_Health.describe()

# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': merged_df_Health.mean(),
    'Median': merged_df_Health.median(),
    'Mode': merged_df_Health.mode().iloc[0],
})
print("\Summary Statistics:")
print(summary_stats)

correlation_matrix = merged_df_Health.corr()
plt.figure(figsize=(5,5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix After Cleaning Data')
plt.show()

merged_df_Health.to_csv('Health.csv')
files.download('Health.csv')

"""# ON SOCIETY ANALSYSIS

This code is created by Yegotom Atta

Contents:

Name of your dataset - Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Name of your dataset - Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Name of your dataset- Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis
"""

from google.colab import files
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import statistics

uploaded = files.upload()

"""## **Public Health**"""

df13 = pd.read_csv('Public health.csv')

print("Before:")
df13.info()

missing_data_per_year = df13.iloc[:, 1:].isnull().sum()

# Create the histogram
plt.figure(figsize=(10, 6))
plt.bar(missing_data_per_year.index, missing_data_per_year.values, color='skyblue')
plt.title('Number of Missing Values per Year')
plt.xlabel('Year')
plt.xticks(rotation=45)
plt.ylabel('Number of Missing Values')
plt.tight_layout()
plt.show()

print("Data Types")
print(df13.dtypes)

duplicates = df13.duplicated()
print("Duplicate Data:")
print(df13[duplicates])

#Remove Albania row no data

df13 = df13[df13['TIME'] != 'Albania']

columns_to_drop = ['2004', '2006', '2008']
df13.drop(columns_to_drop, axis=1, inplace=True)

df13

df13 = df13.reset_index(drop=True)

sns.set(style="whitegrid")

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df13.isnull(), cbar=False, cmap='viridis')
plt.title('Public Health Missing Values Heatmap')
plt.show()

#Fill median for North Macedonia

values200000000000000 = [
    df13.at[35, '2014'],
    df13.at[35, '2016'],
    df13.at[35, '2018'],
    df13.at[35, '2020']
]
median_mac = statistics.median(values200000000000000)
median_mac

df13.at[35, '2012'] = 983.0
df13.at[35, '2010'] = 983.0

#Fill median for Norway

values3 = [
    df13.at[31, '2012'],
    df13.at[31, '2014'],
    df13.at[31, '2016'],
    df13.at[31, '2018'],
    df13.at[31, '2020']
]
median_no = statistics.median(values3)
median_noFill Median for Bosnia and Herzegovina

df13.at[31, '2010'] = 459.0

#Fill Median for Bosnia and Herzegovina

values4 = [
    df13.at[33, '2012'],
    df13.at[33, '2016'],
    df13.at[33, '2018'],
    df13.at[33, '2020']
]
median_bo = statistics.median(values4)
median_bo

df13.at[33, '2010'] = 1.0
df13.at[33, '2014'] = 1.0

#Fill median for kosovo

values5 = [
    df13.at[38, '2014'],
    df13.at[38, '2016'],
    df13.at[38, '2020']
]
median_ko = statistics.median(values5)
median_ko

df13.at[38, '2010'] = 8.0
df13.at[38, '2012'] = 8.0
df13.at[38, '2018'] = 8.0

#Fill median for Liechtenstein

values6 = [
    df13.at[30, '2014'],
    df13.at[30, '2010'],

]
median_lie = statistics.median(values6)
median_lie

df13.at[30, '2012'] = 3.5
df13.at[30, '2016'] = 3.5
df13.at[30, '2018'] = 3.5
df13.at[30, '2020'] = 3.5

#fill average for Montenegro

average2 =(df13.at[34, '2012'] + df13.at[34, '2014'] + df13.at[34, '2016'] + df13.at[34, '2018'] + df13.at[34, '2020']) / 5

average2 = round(average2)
print(average2)

df13.at[34, '2010'] = 692.0

df13

#Fill average for Serbia

average3 =( df13.at[36, '2012'] + df13.at[36, '2014'] + df13.at[36, '2016'] + df13.at[36, '2018'] + df13.at[36, '2020']) / 5

average3 = round(average3)
print(average3)

df13.at[36, '2010'] = 4041.0

#fill average for United Kingdom

average4 = (df13.at[32, '2010'] + df13.at[32, '2012'] + df13.at[32, '2014'] + df13.at[32, '2016'] + df13.at[32, '2018'] ) / 5

average4 = round(average4)
print(average4)

df13.at[32, '2020'] = 411346

#median for European Union - 28 countries (2013-2020)

values9000 = [
    df13.at[1, '2012'],
    df13.at[1, '2010'],

    df13.at[1, '2014'],
    df13.at[1, '2016'],
    df13.at[1, '2018']
]
median_mac = statistics.median(values9000)
median_mac

df13.at[1, '2020'] = 2170000.0

values9900 = [

    df13.at[15, '2012'],
    df13.at[15, '2014'],
    df13.at[15, '2016'],
    df13.at[15, '2018'],
    df13.at[15, '2020'],
]
median_mac = statistics.median(values9900)
median_mac

df13.at[15, '2010'] = 2139.0

values9866011 = [
    df13.at[27, '2012'],
    df13.at[27, '2010'],
    df13.at[27, '2016'],
    df13.at[27, '2018'],
    df13.at[27, '2020']


]
median_mac = statistics.median(values9866011)
median_mac

df13.at[27, '2014'] = 3130.0

df13.at[38, 'TIME'] = 'Kosovo'

df13.at[37, 'TIME'] = 'Türkiye'

df13.rename(columns={'TIME': 'Country'}, inplace=True)

df13

df13.to_csv('Public_health.csv', index=False)
files.download('Public_health.csv')

df13_subset = df13.iloc[2:]
df13_subset_reset = df13_subset.reset_index(drop=True)

# Bar plot showing public health values for specific countries
plt.figure(figsize=(12, 8))
sns.barplot(x='Value', y='Country', data=df13_subset_reset.melt(id_vars='Country', var_name='year', value_name='Value'),
            estimator='mean', ci=None, hue='Country', palette='viridis')
plt.title('Public health values per country')
plt.xlabel('Average public health value')
plt.ylabel('Countries')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""## Environment degradation"""

uploaded = files.upload()

df12 = pd.read_csv('soils.csv')

print("before:")
df12.info()

sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.heatmap(df12.isnull(), cbar=False, cmap='viridis')
plt.title('Soil Data Missing Values Heatmap')
plt.show()

#Remove Albania row no data

df12 = df12[df12['TIME'] != 'Albania']
df12 = df12.reset_index(drop=True)

df12

#Fill average for United Kingdom

average10 = (df12.at[32, '2010'] + df12.at[32, '2012'] + df12.at[32, '2014'] + df12.at[32, '2016']  + df12.at[32, '2018'] ) / 5

average10 = round(average10)
print(average10)

df12.at[32, '2020'] = 55120645.0

#fill in average for European Union - 28 countries (2013-2020)

values9000111111 = [
    df12.at[1, '2010'],
    df12.at[1, '2014'],
    df12.at[1, '2016'],
    df12.at[1, '2018'],
    df12.at[1, '2012']
]
median_mac = statistics.median(values9000111111 )
median_mac

df12.at[1, '2020'] = 466630000.0

#Fill average for Montenegro

average12 = (df12.at[34, '2012'] + df12.at[34, '2014'] + df12.at[34, '2016']  + df12.at[34, '2018'] + df12.at[34, '2020'] ) / 5

average12 = round(average12)
print(average12)

df12.at[34, '2010'] = 123975.0

#Fill average for Bosnia and Herzegovina

average13 = (df12.at[33, '2012'] + df12.at[33, '2014'] + df12.at[33, '2016']  + df12.at[33, '2018'] + df12.at[33, '2020'] ) / 5

average13 = round(average13)
print(average13)

df12.at[33, '2010'] = 28966.0

#Fill average for Norway

average14 = (df12.at[31, '2014'] + df12.at[31, '2016']  + df12.at[31, '2018'] + df12.at[31, '2020'] ) / 4

average14 = round(average14)
print(average14)

df12.at[31, '2010'] = 1382456.0
df12.at[31, '2012'] = 1382456.0

df12.at[38, 'TIME'] = 'Kosovo'

df12.at[37, 'TIME'] = 'Türkiye'

#Fill average for Kosovo

average15=(df12.at[38, '2014'] + df12.at[38, '2016']  + df12.at[38, '2018'] + df12.at[38, '2020'] ) / 4

average15 = round(average15)
print(average15)

df12.at[38, '2010'] = 15331.0
df12.at[38, '2012'] = 15331.0

#Fill average for Malta

average16=(df12.at[19, '2010'] + df12.at[19, '2016']  + df12.at[19, '2018'] + df12.at[19, '2020'] ) / 4

average16 = round(average16)
print(average16)

df12.at[19, '2012'] = 10481.0
df12.at[19, '2014'] = 10481.0

#Median for Serbia

values90001111112 = [
    df12.at[36, '2020'],
    df12.at[36, '2014'],
    df12.at[36, '2016'],
    df12.at[36, '2018'],
    df12.at[36, '2012']
]
median_mac = statistics.median(values90001111112 )
median_mac

df12.at[36, '2010'] =222365.0

values900011111122 = [
    df12.at[35, '2020'],
    df12.at[35, '2014'],
    df12.at[35, '2016'],
    df12.at[35, '2018'],
    df12.at[35, '2012']
]
median_mac = statistics.median(values900011111122 )
median_mac

df12.at[35, '2010'] =2461.0

values9000111111222 = [
    df12.at[30, '2020'],
    df12.at[30, '2014'],
    df12.at[30, '2016'],
    df12.at[30, '2018']
]
median_mac = statistics.median(values9000111111222 )
median_mac

df12.at[30, '2010'] = 471697.0
df12.at[30, '2012'] = 471697.0

values90001111112223 = [
    df12.at[29, '2020'],
    df12.at[29, '2014'],
    df12.at[29, '2016'],
    df12.at[29, '2018']
]
median_mac = statistics.median(values90001111112223 )
median_mac

df12.at[29, '2010'] = 93807.5
df12.at[29, '2012'] = 93807.5

values900011111122234 = [
    df12.at[19, '2020'],
    df12.at[19, '2016']

]
median_mac = statistics.median(values900011111122234 )
median_mac

df12.at[19, '2010'] = 14579.5
df12.at[19, '2012'] = 14579.5
df12.at[19, '2014'] = 14579.5
df12.at[19, '2018'] = 14579.5

values9000111111222344 = [
    df12.at[9, '2020'],
    df12.at[9, '2018'],
    df12.at[9, '2014'],
    df12.at[9, '2016'],
    df12.at[9, '2012']


]
median_mac = statistics.median(values9000111111222344 )
median_mac

df12.at[9, '2010'] = 375503.0

df12.rename(columns={'TIME': 'Country'}, inplace=True)

df12_subset = df12.iloc[2:]
df12_subset_reset = df12_subset.reset_index(drop=True)

# Bar plot showing public health values for specific countries
plt.figure(figsize=(12, 8))
sns.barplot(x='Value', y='Country', data=df12_subset_reset.melt(id_vars='Country', var_name='year', value_name='Value'),
            estimator='mean', ci=None, hue='Country', palette='viridis')
plt.title('Soil values per country')
plt.xlabel('Average Soil value')
plt.ylabel('Countries')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

df12.to_csv('soil.csv', index=False)
files.download('soil.csv')

print("after:")
df12.info()

"""## **Economic cost**"""

uploaded = files.upload()

df11 = pd.read_csv('Discarded vehicles.csv')

print("Before:")
df11.info()

df11

df11 = df11[df11['TIME'] != 'Albania']

df11 = df11[df11['TIME'] != 'Kosovo*']

columns_to_drop = ['2004', '2006', '2008']
df11.drop(columns_to_drop, axis=1, inplace=True)

df11

df11 = df11.reset_index(drop=True)

sns.set(style="whitegrid")

# Plot the heatmap for missing values
plt.figure(figsize=(10, 6))
sns.heatmap(df11.isnull(), cbar=False, cmap='viridis')
plt.title('Discarded Vehicles Missing Values Heatmap')
plt.show()

#fill in average for European Union - 28 countries (2013-2020)

average35=( df11.at[1, '2010']  + df11.at[1, '2012'] + df11.at[1, '2014'] + df11.at[1, '2016'] + df11.at[1, '2018']  ) / 4

average35 = round(average35)
print(average35)

df11.at[1, '2020'] = 8242500

average40 = (df11.at[32, '2010'] + df11.at[32, '2012'] + df11.at[32, '2014'] + df11.at[32, '2016'] + df11.at[32, '2018'] ) / 5

average40 = round(average40)
print(average40)

df11.at[32, '2020'] = 1163641

#fill missing values for Iceland

df11.at[29, '2010'] = 41.0
df11.at[29, '2012'] = 41.0
df11.at[29, '2014'] = 41.0
df11.at[29, '2018'] = 41.0
df11.at[29, '2020'] = 41.0

#Fill median for Liechtenstein

values90001111112223444 = [
    df11.at[30, '2010'],
    df11.at[30, '2014']




]
median_mac = statistics.median(values90001111112223444 )
median_mac

df11.at[30, '2012'] = 5.0
df11.at[30, '2016'] = 5.0
df11.at[30, '2018'] = 5.0
df11.at[30, '2020'] = 5.0

df11

#Fill average for North Macedonia

average42 = ( df11.at[35, '2018'] +  df11.at[35, '2020'] )/ 2

average42 = round(average42)
print(average42)

df11.at[35, '2010'] = 264.0
df11.at[35, '2012'] = 264.0
df11.at[35, '2016'] = 264.0
df11.at[35, '2014'] = 264.0

#Fill average for Montenegro

average45 = ( df11.at[34, '2012'] + df11.at[34, '2014'] + df11.at[34, '2016'] + df11.at[34, '2018'] + df11.at[34, '2020'] ) / 5

average45 = round(average45)
print(average45)

df11.at[34, '2010'] = 113.0

#Fill median for Bosnia and Herzegovina

values900011111122234444 = [
    df11.at[33, '2012'],
    df11.at[33, '2014'],
    df11.at[33, '2016'],
    df11.at[33, '2018']


]
median_mac = statistics.median(values900011111122234444 )
median_mac

df11.at[33, '2010'] = 26.0
df11.at[33, '2020'] = 26.0

#Fill Median for Serbia

values222 = [
    df11.at[36, '2016'],
    df11.at[36, '2018'],
    df11.at[36, '2020']
]
median_mac36 = statistics.median(values222)
median_mac36

df11.at[36, '2010'] = 2006.0
df11.at[36, '2012'] = 2006.0
df11.at[36, '2014'] = 2006.0

values22200 = [
    df11.at[26, '2018'],
    df11.at[26, '2016'],
    df11.at[26, '2020']
]
median_mac36 = statistics.median(values22200)
median_mac36

df11.at[26, '2010'] = 38036.0
df11.at[26, '2012'] = 38036.0
df11.at[26, '2014'] = 38036.0

df11

#fill average for turkiye

average55 = ( df11.at[37, '2010'] + df11.at[37, '2012'] + df11.at[37, '2014'] + df11.at[37, '2018'] ) / 4

average55 = round(average55)
print(average55)

df11.at[37, '2016'] = 352.0
df11.at[37, '2020'] = 352.0

#Fill average for finland

average360=(df11.at[27, '2010'] + df11.at[27, '2012'] + df11.at[27, '2014'] + df11.at[27, '2016'] ) / 4

average360 = round(average360)
print(average360)

df11.at[27, '2018'] = 71108
df11.at[27, '2020'] = 71108

#Fill average for Portgual

average401 = ( df11.at[23, '2010'] + df11.at[23, '2012'] + df11.at[23, '2014'] + df11.at[23, '2016'] ) / 4

average401 = round(average401)
print(average401)

df11.at[23, '2020'] = 84490
df11.at[23, '2018'] = 84490

#Fill average for Poland

average999 = (df11.at[22, '2010'] + df11.at[22, '2014'] ) / 2

average999 = round(average999)
print(average999)

df11.at[22, '2016'] = 2977
df11.at[22, '2012'] = 2977
df11.at[22, '2020'] = 2977
df11.at[22, '2018'] = 2977

values90001111112223444400= [
    df11.at[3, '2012'],
    df11.at[3, '2010'],
    df11.at[3, '2014'],
    df11.at[3, '2016']


]
median_mac = statistics.median(values90001111112223444400 )
median_mac

df11.at[3, '2018'] = 274.0
df11.at[3, '2020'] = 274.0

values900011111122234444000= [
    df11.at[4, '2010'],
    df11.at[4, '2014'],
    df11.at[4, '2016'],
    df11.at[4, '2012']

]
median_mac = statistics.median(values900011111122234444000 )
median_mac

df11.at[4, '2020'] = 3660.5
df11.at[4, '2018'] = 3660.5

values9000111111222344440000 = [
     df11.at[8, '2010'],
     df11.at[8, '2014'],
     df11.at[8, '2016'],
     df11.at[8, '2012'],
     df11.at[8, '2018']
]
median_mac = statistics.median(values9000111111222344440000 )
median_mac

df11.at[8, '2020'] = 45578.0

values90001111112223444400000 = [
     df11.at[9, '2010'],
     df11.at[9, '2012']
]
median_mac = statistics.median(values90001111112223444400000 )
median_mac

df11.at[9, '2020'] = 70477.0
df11.at[9, '2016'] = 70477.0
df11.at[9, '2014'] = 70477.0
df11.at[9, '2018'] = 70477.0

values9000111111222344440000000011 = [
     df11.at[18, '2010'],
     df11.at[18, '2018'],
     df11.at[18, '2016'],
     df11.at[18, '2012'],
     df11.at[18, '2020']
]
median_mac = statistics.median(values9000111111222344440000000011 )
median_mac

df11.at[18, '2014'] = 19145.0

#fill median for Croatia

values9000111111222344440000000011111111111 = [

     df11.at[12, '2018'],
     df11.at[12, '2016'],
     df11.at[12, '2012'],
     df11.at[12, '2020']
]
median_mac = statistics.median(values9000111111222344440000000011111111111 )
median_mac

df11.at[12, '2010'] = 31040.0

values900011111122234444000000001111111111110 = [

     df11.at[31, '2018'],
     df11.at[31, '2016'],
     df11.at[31, '2014'],
     df11.at[31, '2010'],
     df11.at[31, '2020']
]
median_mac = statistics.median(values900011111122234444000000001111111111110 )
median_mac

df11.at[31, '2012'] = 216205.0

values900011111122234444000000001111111111111 = [


     df11.at[7, '2016'],
     df11.at[7, '2014'],
     df11.at[7, '2020']
]
median_mac = statistics.median(values900011111122234444000000001111111111111 )
median_mac

df11.at[7, '2010'] = 12709.0
df11.at[7, '2012'] = 12709.0
df11.at[7, '2018'] = 12709.0

values90001111112223444400000000111111111111111 = [


     df11.at[19, '2018'],
     df11.at[19, '2012'],
     df11.at[19, '2020']
]
median_mac = statistics.median(values90001111112223444400000000111111111111111 )
median_mac

df11.at[19, '2010'] = 13945.0
df11.at[19, '2014'] = 13945.0
df11.at[19, '2016'] = 13945.0

df11

df11.rename(columns={'TIME': 'Country'}, inplace=True)

print("After:")
df11.info()

df11_subset = df11.iloc[2:]
df11_subset_reset = df11_subset.reset_index(drop=True)

# Bar plot showing public health values for specific countries
plt.figure(figsize=(12, 8))
sns.barplot(x='Value', y='Country', data=df11_subset_reset.melt(id_vars='Country', var_name='year', value_name='Value'),
            estimator='mean', ci=None, hue='Country', palette='viridis')
plt.title('Discarded vechiles values per country')
plt.xlabel('Average Discarded vechiles value')
plt.ylabel('Countries')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

df11.to_csv('Discarded vehicles.csv', index=False)
files.download('Discarded vehicles.csv')

"""## Organisation and integration for Society"""

uploaded = files.upload()

df13 = pd.read_csv('Public_health (7).csv')

df13

uploaded = files.upload()

df12

df12 = pd.read_csv('soil (6).csv')

uploaded = files.upload()

df11 = pd.read_csv('Discarded vehicles (6).csv')

df11

publichealth_long = df13.reset_index().melt(id_vars=['Country'], var_name='year', value_name='Public health')

Soil_long = df12.reset_index().melt(id_vars=['Country'], var_name='year', value_name='Soil')


discardedvechiles_long = df11.reset_index().melt(id_vars=['Country'], var_name='year', value_name='Discarded vechiles')


merged_df = pd.merge(publichealth_long, Soil_long, on=['Country', 'year'])


merged_df = pd.merge(merged_df, discardedvechiles_long, on=['Country', 'year'])

publichealth_long = df13.reset_index(drop=True).melt(id_vars=['Country'], var_name='year', value_name='Public health')


Soil_long = df12.reset_index(drop=True).melt(id_vars=['Country'], var_name='year', value_name='Soil')

discardedvechiles_long = df11.reset_index(drop=True).melt(id_vars=['Country'], var_name='year', value_name='Discarded vechiles')


merged_df = pd.merge(publichealth_long, Soil_long, on=['Country', 'year'])


merged_df = pd.merge(merged_df, discardedvechiles_long, on=['Country', 'year'])

merged_df

merged_df.to_csv('society merged dataset.csv', index=False)
files.download('society merged dataset.csv')

"""# ON CARBON INTENSITY ANALYSIS

This code is created by Arman Somai

Contents:

Name of your dataset - Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Name of your dataset - Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis

Name of your dataset- Data Exploration, Data Cleaning, Data Organisation, Sensitive & Stastical Analysis
"""

from google.colab import files
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import statistics

"""## Disposal of Plastic"""

uploaded = files.upload()

df16 = pd.read_csv('Disposal of plastic.csv')

df16.drop(df16.columns[[1,2,3]], axis=1, inplace=True)

df16

#changing 'TIME' column to 'COUNTRY

df16.rename(columns={'TIME': 'COUNTRIES'}, inplace=True)
df16.info()

#DATA EXPLORATION FOR DISPOSAL OF PLASTIC

df16.describe()

duplicates = df16[df16.duplicated()]
print("Duplicate data:")
print(duplicates)

missing_values = df16.isnull().sum()
print("Missing Values:")
print(missing_values)

#HISTOGRAM SHOWING MISSING VALUES

plt.figure(figsize=(5,5))
missing_values.iloc[1:].plot(kind='bar')
plt.title('Count of Missing Values from 2010 to 2020')
plt.xlabel('Years')
plt.ylabel('Missing Values Count')
plt.show()
print()

#List of missing data per country
missing_per_country = df16.isnull().sum(axis=1)
missing_per_country.index = df16['COUNTRIES']
print("Missing values per country:")
print(missing_per_country)

# Heatmap for missing values per country
plt.figure(figsize=(5,5))
sns.heatmap(df16.iloc[:, 1:].isnull(),cmap='viridis', cbar=False, yticklabels=df16['COUNTRIES'])
plt.title('Heatmap of missing values')
plt.show()

#OUTLIERS DETECTION

numeric_columns = df16.select_dtypes(include=['int', 'float']).columns
plt.figure(figsize=(5,5))
sns.boxplot(data=df16.iloc[:,1:][numeric_columns], palette='viridis')
plt.title('Boxplot for outliers')
plt.show()

#DATA CLEANING FOR DISPOSAL PLASTIC

df16 = df16.drop(21)
df16 = df16.drop(30)
df16 = df16.drop(36)

df16 = df16.reset_index(drop=True)

#PUT THE VALUE OF 2018 FOR 2020 FOR UNION EUROPEAN SINCE THE VALUES ARE MASSIVE AND RISE GRADUALLY

df16.at[1, '2020'] = 80000.0

#FILL AVERAGE FOR UNITED KINGDOM

average_uk = (df16.at[30, '2012'] + df16.at[30, '2014'] + df16.at[30, '2016'] + df16.at[30, '2018']) / 8
df16.at[30, '2020'] = average_uk
df16

#Fill median for bosnia

values = [
    df16.at[31, '2012'],
    df16.at[31, '2014'],
    df16.at[31, '2016'],
    df16.at[31, '2018'],
    df16.at[31, '2020']
]
median_bosnia = statistics.median(values)

df16.at[31,'2010'] = median_bosnia

#FILL MEDIAN MONTENEGRO

values1 = [
    df16.at[32, '2012'],
    df16.at[32, '2014'],
    df16.at[32, '2016'],
    df16.at[32, '2018'],
    df16.at[32, '2020']
]
median_mont = statistics.median(values1)
median_mont
df16.at[32, '2010'] = median_mont

#FILL MEDIAN ON KOSOVO

values3 = [
    df16.at[36, '2012'],
    df16.at[36, '2014'],
    df16.at[36, '2016'],
    df16.at[36, '2018'],
    df16.at[36, '2020']
]
median_kos = statistics.median(values3)

df16.at[36, '2010'] = median_kos

#NEW METRICS

missing_values = df16.isnull().sum()
missing_values

# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': df16.mean(),
    'Median': df16.median(),
    'Mode': df16.mode().iloc[0],
})
print("\Summary Statistics:")
print(summary_stats)

#Correlation metrics after cleaning data

correlation_matrix = df16.corr()
plt.figure(figsize=(5,5))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix After Cleaning Data')
plt.show()

#Wastage per country

df16['Total Wastage'] = df16.iloc[:, 1:].sum(axis=1)
plt.figure(figsize=(10,8))
sns.barplot(x='Total Wastage', y='COUNTRIES', data=df16, palette='viridis')
plt.title('Total Wastage per Country')
plt.xlabel('Total Wastage')
plt.ylabel('Country')
plt.show()

df16.to_csv('Disposal of plastic.csv', index=False)
files.download('Disposal of plastic.csv')

"""##Landfill"""

uploaded = files.upload()

df14 = pd.read_csv('Landfill.csv')

df14

df14.drop(df14.columns[[1,2,3]], axis=1, inplace=True)

df14.rename(columns={'TIME': 'COUNTRIES'}, inplace=True)

#DATA EXPLORATION FOR PLASTIC WASTE

duplicates = df14[df14.duplicated()]
print("Duplicate data:")
print(duplicates)

missing_values = df14.isnull().sum()
print("Missing Values:")
print(missing_values)

plt.figure(figsize=(5,5))
missing_values.iloc[1:].plot(kind='bar')
plt.title('Count of Missing Values from 2010 to 2020')
plt.xlabel('Years')
plt.ylabel('Missing Values Count')
plt.show()
print()

#List of missing data per country
missing_per_country = df14.isnull().sum(axis=1)
missing_per_country.index = df14['COUNTRIES']
print("Missing values per country:")
print(missing_per_country)

# Heatmap for missing values per country
plt.figure(figsize=(5,5))
sns.heatmap(df14.iloc[:, 1:].isnull(),cmap='viridis', cbar=False, yticklabels=df14['COUNTRIES'])
plt.title('Heatmap of missing values')
plt.show()

# Outliers detection
numeric_columns = df14.select_dtypes(include=['int', 'float']).columns
plt.figure(figsize=(5,5))
sns.boxplot(data=df14.iloc[:,1:][numeric_columns], palette='viridis')
plt.title('Boxplot for outliers')
plt.show()

#CLEANING FOR LANDFILL

missing_values = df14.isnull().sum()
missing_values

missing_pct = round(df14.isnull().sum()/len(df14)*100,1)
print(missing_pct)

df14.describe()

#DROP ROWS SINCE NOW DATA

df14 = df14.drop(39)
df14 = df14.drop(36)
df14

df14 = df14.reset_index(drop=True)

#putting the value of 2018 in 2020 as it raise gradually

df14.at[1, '2020'] = 5850000.0
df14.at[32, '2020'] = 178178.0

#Replacing the missing values with the median (BOSNIA)

values = [
    df14.at[33, '2012'],
    df14.at[33, '2014'],
    df14.at[33, '2016'],
    df14.at[33, '2018'],
    df14.at[33, '2020']
]
median = statistics.median(values)
median = round(median)
df14.at[33, '2010'] = median

#Replacing with the median in the row of Montenegro

average_mon = (df14.at[34, '2012'] + df14.at[34, '2014'] + df14.at[34, '2016'] + df14.at[34, '2018'] + df14.at[34, '2020']) / 5
average_mon = round(average_mon)

df14.at[34, '2010'] = average_mon

#NEW METRICS

missing_values = df14.isnull().sum()
missing_values

# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': df14.mean(),
    'Median': df14.median(),
    'Mode': df14.mode().iloc[0],
})
print("\Summary Statistics:")
print(summary_stats)

#Correlation metrics after cleaning data

correlation_matrix = df14.corr()
plt.figure(figsize=(8,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix After Cleaning Data')
plt.show()

#Wastage per country

df14['Total Wastage'] = df14.iloc[:, 1:].sum(axis=1)

plt.figure(figsize=(10,8))
sns.barplot(x='Total Wastage', y='COUNTRIES', data=df14, palette='viridis')
plt.title('Total Wastage per Country')
plt.xlabel('Total Wastage')
plt.ylabel('Country')
plt.show()

df14.to_csv('Landfill.csv', index=False)
files.download('Landfill.csv')

"""## Waste Incineration"""

uploaded = files.upload()

df15 = pd.read_csv('Waste incineration.csv')

df15.drop(df15.columns[[1,2,3]], axis=1, inplace=True)

df15

df15.rename(columns={'TIME': 'COUNTRIES'}, inplace=True)
df15.info()

#DATA EXPLORATION FOR MEDICAL WASTE

duplicates = df15[df15.duplicated()]
print("Duplicate data:")
print(duplicates)

missing_values = df15.isnull().sum()
print("Missing Values:")
print(missing_values)

#Histogram to show missing values

plt.figure(figsize=(5,5))
missing_values.iloc[1:].plot(kind='bar')
plt.title('Count of Missing Values from 2010 to 2020')
plt.xlabel('Years')
plt.ylabel('Missing Values Count')
plt.show()
print()

#List of missing data per country
missing_per_country = df15.isnull().sum(axis=1)
missing_per_country.index = df15['COUNTRIES']
print("Missing values per country:")
print(missing_per_country)

# Heatmap for missing values per country
plt.figure(figsize=(5,5))
sns.heatmap(df15.iloc[:, 1:].isnull(),cmap='viridis', cbar=False, yticklabels=df15['COUNTRIES'])
plt.title('Heatmap of missing values')
plt.show()

# Outliers detection
numeric_columns = df15.select_dtypes(include=['int', 'float']).columns
plt.figure(figsize=(5,5))
sns.boxplot(data=df15.iloc[:,1:][numeric_columns], palette='viridis')
plt.title('Boxplot for outliers')
plt.show()

#DATA CLEANING FOR MEDICAL WASTE

missing_values = df15.isnull().sum()
missing_values

missing_pct = round(df15.isnull().sum()/len(df15)*100,1)
print(missing_pct)

df15.describe()

#Drop rows with no values

df15 = df15.drop(30)
df15 = df15.drop(36)
df15 = df15.drop(39)

df15 = df15.reset_index(drop=True)

#PUTTING THE VALUE OF 2018 IN 2020 AS IT RAISE GRADUALLY

df15.at[31, '2020'] =168644.0
df15.at[1, '2020'] = 12460000.0
df15

#FILL WITH MEDIAN ON BOSNIA

values1 = [
    df15.at[32, '2012'],
    df15.at[32, '2014'],
    df15.at[32, '2016'],
    df15.at[32, '2018'],
    df15.at[32, '2020']
]
median_B = statistics.median(values1)
median_B

df15.at[32, '2010'] = median_B

#FILL WITH MEDIAN ON MONTENEGRO

values1 = [
    df15.at[33, '2012'],
    df15.at[33, '2014'],
    df15.at[33, '2016'],
    df15.at[33, '2018'],
    df15.at[33, '2020']
]
median_M = statistics.median(values1)
median_M

df15.at[33, '2010'] = median_M
df15

df15.to_csv('Waste incineration.csv', index=False)
files.download('Waste incineration.csv')

# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': df15.mean(),
    'Median': df15.median(),
    'Mode': df15.mode().iloc[0],
})
print("\Summary Statistics:")
print(summary_stats)

#Correlation metrics after cleaning data

correlation_matrix = df15.corr()
plt.figure(figsize=(8,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix After Cleaning Data')
plt.show()

#Wastage per country

df15['Total Wastage'] = df15.iloc[:, 1:].sum(axis=1)

plt.figure(figsize=(10,8))
sns.barplot(x='Total Wastage', y='COUNTRIES', data=df15, palette='viridis')
plt.title('Total Wastage per Country')
plt.xlabel('Total Wastage')
plt.ylabel('Country')
plt.show()

#New metrics

missing_values = df15.isnull().sum()
missing_values

"""## Organisataion and integrations for Carbon Intensity




"""

df14 = pd.read_csv("Landfill.csv")
df15 = pd.read_csv("Waste incineration.csv")
df16 = pd.read_csv("Disposal of plastic.csv")

Landfill_waste_long = df14.reset_index().melt(id_vars = "COUNTRIES", var_name = "year", value_name = "Landfill Waste")


Incineration_waste_long = df15.reset_index().melt(id_vars = "COUNTRIES", var_name = "year", value_name = "Waste incineration")


Disposal_waste_long = df16.reset_index().melt(id_vars = "COUNTRIES", var_name = "year", value_name = "Disposal of plastic Waste")

merged_df_Carbon = pd.merge(Incineration_waste_long,Disposal_waste_long,on=['COUNTRIES','year'])


merged_df_Carbon = pd.merge(merged_df_Carbon,Incineration_waste_long, on=['COUNTRIES','year'])


merged_df_Carbon = pd.merge(merged_df_Carbon,Landfill_waste_long,on=['COUNTRIES','year'])

merged_df_Carbon

merged_df_Carbon.describe()

# Summary statistics (mean, median, mode)
summary_stats = pd.DataFrame({
    'Mean': merged_df_Carbon.mean(),
    'Median': merged_df_Carbon.median(),
    'Mode': merged_df_Carbon.mode().iloc[0],
})
print("\Summary Statistics:")
print(summary_stats)

correlation_matrix = merged_df_Carbon.corr()
plt.figure(figsize=(8,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix After Cleaning Data')
plt.show()
